\begin{exercisebox}{2 Proof of Proposition 1 Chapter 1}

  Let $X$ be the design matrix of size $n\times(p+1)$. Assume $X$ is full rank, i.e., $\mathrm{rank}(X)=p+1$. Consider the linear model
  \[
    Y = X\beta + \varepsilon,
  \]
  with $\beta\in\mathbb{R}^{p+1}$. Define the ordinary least squares estimator (OLSE)
  \[
    \widehat{\beta} = \arg\min_{\beta\in\mathbb{R}^{p+1}} \|Y - X\beta\|^2.
  \]

  \begin{enumerate}
    \item Show that
    \[
      \widehat{\beta} = \widehat{\beta}(Y) = (X^{\top}X)^{-1}X^{\top}Y.
    \]
    \item Application for $p=1$:
    let $(x_1,y_1),\ldots,(x_n,y_n)$ be $n$ pairs of real numbers. Determine the real
    $\hat a$ and $\hat b$ that minimize
    \[
      \mathrm{RSS}(a,b) = \sum_{i=1}^n \bigl(y_i - a - b x_i\bigr)^2.
    \]
    Interpret the result.
  \end{enumerate}
\end{exercisebox}

\begin{answerenum}
    \item We want to minimize the function \( \beta \mapsto \| Y - X\beta \|^2 \).
        \[
          \| Y - X\beta \| \leq \| Y \| + \| X\beta \| \leq \|Y\| + C \dot \|\beta\|
        \]
        As this shows that the function goes to infinity as \( \|\beta\| \) goes to infinity, the minimum is attained at some point \( \widehat{\beta} \). The function is differentiable and convex, so the minimum is attained at a point where the gradient is zero. 
        
        calculating the gradient, we have
        \[
          \nabla_\beta \| Y - X\beta \|^2 = -2X^{\top}(Y - X\beta)
        \]
        Setting this to zero, we have
        \[
            X^{\top}Y = X^{\top}X\widehat{\beta}
        \]
        As \( X \) is full rank, \( X^{\top}X \) is invertible, and we have
        \[
          \widehat{\beta} = (X^{\top}X)^{-1}X^{\top}Y
        \]
\end{answerenum}