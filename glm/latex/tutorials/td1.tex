% TD 1: Linear regression model exercises
% Converted from PDF to LaTeX exercise format

\begin{exercise}[Unbiased estimator of \(\sigma^2\) in the non-Gaussian model]
Consider the following non-Gaussian linear model:
\[Y = X\beta + \epsilon\]
with \(\beta \in \mathbb{R}^p\), \(X\) of full rank, and the \(\epsilon_i\) independent, centered and of variance \(\sigma^2\). We pose:
\[\hat{\sigma}^2 = \frac{1}{n - p} \|Y - X\hat{\beta}\|^2\]
We note \(\text{Tr}(\cdot)\) the trace of a matrix.

\begin{enumerate}
    \item Show that \((n - p)\hat{\sigma}^2 = \text{Tr}(\epsilon^{\top}P_{X^{\perp}} \epsilon)\)

    \item Using the fact that \(\text{Tr}(AB) = \text{Tr}(BA)\) for \(A\) and \(B\) of respective size \((m \times n)\) and \((n \times m)\), show that
    \[(n - p)E_{\beta}[\hat{\sigma}^2] = \sigma^2 \text{Tr}(P_{X^{\perp}})\]

    \item Deduce that \(E_{\beta}[\hat{\sigma}^2] = \sigma^2\).
\end{enumerate}
\end{exercise}

\begin{exercise}[Proof of theorem 4 chapter 4]
Consider the following Gaussian linear model \(Y = X\beta + \epsilon\) where \(\beta \in \mathbb{R}^r\), \(X\) is a full rank matrix of size \(n \times r\) (\(n > r\)). Let \(C \in M_{q,r}(\mathbb{R})\). We want to test
\[H_0 : C\beta = 0_q \quad \text{versus} \quad H_1 : C\beta \neq 0_q\]
We assume that \(\text{rg}(C) = q \leq r\). Therefore, you will note that \(\text{rg}(C^{\top}) = q\) where \(C^{\top}\) is the transpose of \(C\).

\begin{enumerate}
    \item Show that if \(Z \sim N_q(0_q, \Sigma)\) then \(Z^{\top}\Sigma^{-1}Z \sim \chi^2_q\).

    \item Show that \(C(X^{\top}X)^{-1}C^{\top}\) is a symmetric and invertible matrix.

    \item Recall the ordinary least squares expression \(\hat{\beta}\).

    \item What is the law of \(\hat{\beta}\)?

    \item Deduce the law of \(C\hat{\beta}\) under the hypothesis \(H_0\).

    \item Deduce that, under \(H_0\),
    \[R = \frac{(C\hat{\beta})^{\top}(C(X^{\top}X)^{-1}C^{\top})^{-1}(C\hat{\beta})}{\sigma^2} \sim \chi^2_q\]

    \item Conclude that, under \(H_0\),
    \[F = \frac{\hat{\beta}^{\top}C^{\top}(C(X^{\top}X)^{-1}C^{\top})^{-1}C\hat{\beta}}{q\hat{\sigma}^2}\]
    is distributed according to a Fisher distribution with \((q, n - r)\) degrees of freedom. Each step of the reasoning must be carefully justified.

    \item Justify and construct a test of \(H_0\) against \(H_1\) of level \(\alpha\).
\end{enumerate}
\end{exercise}

\begin{exercise}[MCQ]
We have observations \((x_i, y_i) \in \mathbb{R}^2\), \(\forall i = 1, \ldots, n\). We consider the following classical Gaussian linear model:
\[Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \quad \forall i = 1, \ldots, n\]
where \((\beta_0, \beta_1) \in \mathbb{R}^2\) and \(\varepsilon_i \sim N(0, \sigma^2)\) are i.i.d.

Let \(X = \begin{pmatrix}
1 & x_1 \\
\vdots & \vdots \\
1 & x_n
\end{pmatrix}\). Assume \(X\) is a full rank matrix and note \(\hat{\beta}_0\) and \(\hat{\beta}_1\) the least squares estimators of \(\beta_0\) and \(\beta_1\).

For each of the following questions, give the answer.

\begin{enumerate}
    \item Are the variables \(Y_i\) independent and identically distributed?
    \begin{itemize}
        \item[a)] Yes \quad b) No \quad c) not always
    \end{itemize}

    \item Does the regression line calculated on the observations pass through the mean point \((\bar{x}, \bar{y})\)?
    \begin{itemize}
        \item[a)] Yes \quad b) No \quad c) Only if I am lucky
    \end{itemize}

    \item Is it possible to find estimators of \(\beta_0\) and \(\beta_1\) with smaller variance than the ordinary least squares estimators?
    \begin{itemize}
        \item[a)] Yes \quad b) No \quad c) Maybe.
    \end{itemize}

    \item Are \(\hat{\beta}_0\) and \(\hat{\beta}_1\) independent?
    \begin{itemize}
        \item[a)] Yes \quad b) No \quad c) It depends on the matrix \(X\)
    \end{itemize}

    \item If the coefficient of determination \(R^2\) calculated on the observations is equal to 1, are the points \((x_i, y_i)_{i=1,\ldots,n}\) aligned?
    \begin{itemize}
        \item[a)] Yes \quad b) No \quad c) Not necessarily
    \end{itemize}

    \item Are \(\hat{Y}\) and \(Y - \hat{Y}\) independent?
    \begin{itemize}
        \item[a)] Yes \quad b) No \quad c) It depends on the matrix \(X\)
    \end{itemize}

    \item Are \(\bar{Y} = \frac{\sum_{i=1}^n Y_i}{n}\) and \(Y - \hat{Y}\) independent?
    \begin{itemize}
        \item[a)] Yes \quad b) No \quad c) It depends on the matrix \(X\)
    \end{itemize}

    \item Is the maximum likelihood estimator of \(\sigma^2\) unbiased?
    \begin{itemize}
        \item[a)] Yes \quad b) No \quad c) We don't know
    \end{itemize}
\end{enumerate}
\end{exercise}

\begin{exercise}[This exercise will be solved without the tools of linear algebra]
Let \((x_1, y_1), \ldots, (x_n, y_n)\) be \(n\) pairs of real numbers. We suppose that \(y_i\) are the realization of \(Y_i\) whose law is given by the following equation:
\[Y_i = a + bx_i + \varepsilon_i, \quad \varepsilon_i \sim_{i.i.d.} N(0, \sigma^2)\]

\begin{enumerate}
    \item Determine \(\hat{A}\) and \(\hat{B}\) the maximum likelihood estimators of \(a\) and \(b\). Interpret the estimators.

    \item Show that these estimators are unbiased.

    \item Calculate the variance of the estimators \(\text{Var}_{\beta}(\hat{A})\) and \(\text{Var}_{\beta}(\hat{B})\). How do these variances vary as a function of \(\sigma^2\) and the experimental design \(x_1, \ldots, x_n\)?

    \item Compute the covariance of \(\hat{A}\) and \(\hat{B}\). Comment.

    \item Let \(\hat{Y}_i = \hat{A} + \hat{B}x_i\) and \(\hat{\varepsilon}_i = Y_i - \hat{Y}_i\). Show that \(\sum_{i=1}^n \hat{\varepsilon}_i = 0\).

    \item Show that \(\frac{\sum_{i=1}^n \hat{\varepsilon}_i^2}{n-2}\) is an unbiased estimator of \(\sigma^2\).

    \item Let \(x_{n+1}\) be another value. We define \(\hat{Y}_{n+1} = \hat{A} + \hat{B}x_{n+1}\). Compute the variance of this prediction.

    \item Furthermore, let \(Y_{n+1} = A + Bx_{n+1} + \varepsilon_{n+1}\). Calculate the variance of \(\hat{\varepsilon}_{n+1} = Y_{n+1} - \hat{Y}_{n+1}\). Compare it to the variance of \(\varepsilon_i\) (for \(i = 1, \ldots, n\)).

    \item Gauss-Markov Theorem:
    \begin{enumerate}
        \item[(a)] Show that \(\hat{B}\) is written as a linear combination of the observations (we will explain the weights).

        \item[(b)] Consider \(\tilde{B} = \sum_{i=1}^n \lambda_i Y_i\) another unbiased estimator of \(B\), written as a linear combination of \(Y_i\). Show that \(\sum_{i=1}^n \lambda_i = 0\) and \(\sum_{i=1}^n \lambda_i x_i = 1\).

        \item[(c)] Deduce that \(\text{Var}_{\beta}(\tilde{B}) \geq \text{Var}_{\beta}(\hat{B})\)
    \end{enumerate}
\end{enumerate}
\end{exercise}