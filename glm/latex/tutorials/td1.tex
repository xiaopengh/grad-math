% TD 1: Linear regression model exercises
% Converted from PDF to LaTeX exercise format

\begin{exercise}[Proof of Cochran's theorem]
Let \(Z\) be a Gaussian random vector in \(\mathbb{R}^n\) with \(Z \sim N(\mu, \sigma^2 I_n)\), where \(\mu \in \mathbb{R}^n\) and \(\sigma > 0\). Let \(F_1, \ldots, F_m\) be subspaces of dimension \(d_i\), orthogonal to each other such that \(\mathbb{R}^n = F_1 \oplus \cdots \oplus F_m\). For \(i = 1, \ldots, m\), let \(P_{F_i}\) denote the orthogonal projection matrix onto \(F_i\). Prove that

\begin{enumerate}
    \item The random vectors \(P_{F_1}Z, \ldots, P_{F_m}Z\) have respective distributions
    \begin{align}
        &N(P_{F_1}\mu, \sigma^2 P_{F_1}), \ldots, N(P_{F_m}\mu, \sigma^2 P_{F_m})
    \end{align}

    \item The random vectors \(P_{F_1}Z, \ldots, P_{F_m}Z\) are pairwise independent.

    \item The random variables
    \begin{align}
        &\frac{\|P_{F_1}(Z-\mu)\|^2}{\sigma^2}, \ldots, \frac{\|P_{F_m}(Z-\mu)\|^2}{\sigma^2}
    \end{align}
    have respective distributions \(\chi^2(d_1), \ldots, \chi^2(d_m)\).

    \item The random variables 
    \begin{align}
        &\frac{\|P_{F_1}(Z-\mu)\|^2}{\sigma^2}, \ldots, \frac{\|P_{F_m}(Z-\mu)\|^2}{\sigma^2}
    \end{align}
    are pairwise independent.
\end{enumerate}
\end{exercise}

\begin{exercise}[Proof of Proposition 1. of the chapter 1]
Let \(X\) be the design matrix of size \(n \times (p + 1)\). We assume \(X\) to be full rank (\(\text{rank}(X) = p + 1\)). Let define the following linear model
\[Y = X\beta + \epsilon\]
with \(\beta \in \mathbb{R}^{p+1}\). Let
\[\hat{\beta} = \arg \min_{\beta \in \mathbb{R}^{p+1}} \|Y - X\beta\|^2\]
be the ordinary least square estimator (OLSE).

\begin{enumerate}
    \item Show that OLSE exists and is unique such that
    \[\hat{\beta} = \hat{\beta}(Y) = (X^{\top}X)^{-1}X^{\top}Y\]

    \item Application for \(p = 1\): Let \((x_1, y_1), \ldots, (x_n, y_n)\) be \(n\) pairs of real numbers. Determine the real \(\hat{a}\) and \(\hat{b}\) that minimize \(\text{RSS}(a, b) = \sum_{i=1}^n (y_i - a - bx_i)^2\). Interpret.
\end{enumerate}
\end{exercise}

\begin{exercise}
Let \(X\) be a \(n \times p\) matrix of rank \(p\). Let \(\hat{Y}\) be the orthogonal project on the space \([X]\) generated by the column vectors of \(X\) of a vector \(Y\) of \(\mathbb{R}^n\). Show that \(\sum_{i=1}^n (Y_i - \hat{Y}_i) = 0\) if one of the column vectors of \(X\) is the vector \(\mathbf{1}_n = (1, \ldots, 1)\). Interpret.
\end{exercise}

\begin{exercise}
We consider the following simple linear regression statistical model: \(Y_i = \beta x_i + \varepsilon_i\), for \(i = 1, \ldots, n\) where the \(\varepsilon_i\) are independent, centered, of constant variance. We define two estimators of \(\beta \in \mathbb{R}\):
\[\hat{\beta} = \frac{\sum_{i=1}^n x_i Y_i}{\sum_{i=1}^n x_i^2} \quad \text{and} \quad \beta^{\star} = \frac{\sum_{i=1}^n Y_i}{\sum_{i=1}^n x_i}\]

\begin{enumerate}
    \item What is the logic of construction of these estimators?
    \item Show that they are unbiased estimators of \(\beta\).
    \item Compare the variances of these two estimators.
\end{enumerate}
\end{exercise}

\begin{exercise}[An important result]
We consider the Gaussian linear regression model:
\[Y = X\beta + \epsilon, \quad \epsilon \sim N(0_n, \sigma^2 I_n)\]
where \(\beta \in \mathbb{R}^r\), \(Y \in \mathbb{R}^n\) and \(X\) matrix of size \(n \times r\) of rank \(r\).

\begin{enumerate}
    \item Recall the matrix closed form of the OLSE and give an unbiased estimator of \(\sigma^2 > 0\).
    \item Compute the maximum likelihood estimators of \(\beta\) and \(\sigma^2\).
    \item Conclude.
\end{enumerate}
\end{exercise}

\begin{exercise}[Unbiased estimator of \(\sigma^2\) in the non-Gaussian model]
Consider the following non-Gaussian linear model:
\[Y = X\beta + \epsilon\]
with \(\beta \in \mathbb{R}^p\), \(X\) of full rank, and the \(\epsilon_i\) independent, centered and of variance \(\sigma^2\). We pose:
\[\hat{\sigma}^2 = \frac{1}{n - p} \|Y - X\hat{\beta}\|^2\]
We note \(\text{Tr}(\cdot)\) the trace of a matrix.

\begin{enumerate}
    \item Show that \((n - p)\hat{\sigma}^2 = \text{Tr}(\epsilon^{\top}P_{X^{\perp}} \epsilon)\)

    \item Using the fact that \(\text{Tr}(AB) = \text{Tr}(BA)\) for \(A\) and \(B\) of respective size \((m \times n)\) and \((n \times m)\), show that
    \[(n - p)E_{\beta}[\hat{\sigma}^2] = \sigma^2 \text{Tr}(P_{X^{\perp}})\]

    \item Deduce that \(E_{\beta}[\hat{\sigma}^2] = \sigma^2\).
\end{enumerate}
\end{exercise}

\begin{exercise}[Proof of theorem 4 chapter 4]
Consider the following Gaussian linear model \(Y = X\beta + \epsilon\) where \(\beta \in \mathbb{R}^r\), \(X\) is a full rank matrix of size \(n \times r\) (\(n > r\)). Let \(C \in M_{q,r}(\mathbb{R})\). We want to test
\[H_0 : C\beta = 0_q \quad \text{versus} \quad H_1 : C\beta \neq 0_q\]
We assume that \(\text{rg}(C) = q \leq r\). Therefore, you will note that \(\text{rg}(C^{\top}) = q\) where \(C^{\top}\) is the transpose of \(C\).

\begin{enumerate}
    \item Show that if \(Z \sim N_q(0_q, \Sigma)\) then \(Z^{\top}\Sigma^{-1}Z \sim \chi^2_q\).

    \item Show that \(C(X^{\top}X)^{-1}C^{\top}\) is a symmetric and invertible matrix.

    \item Recall the ordinary least squares expression \(\hat{\beta}\).

    \item What is the law of \(\hat{\beta}\)?

    \item Deduce the law of \(C\hat{\beta}\) under the hypothesis \(H_0\).

    \item Deduce that, under \(H_0\),
    \[R = \frac{(C\hat{\beta})^{\top}(C(X^{\top}X)^{-1}C^{\top})^{-1}(C\hat{\beta})}{\sigma^2} \sim \chi^2_q\]

    \item Conclude that, under \(H_0\),
    \[F = \frac{\hat{\beta}^{\top}C^{\top}(C(X^{\top}X)^{-1}C^{\top})^{-1}C\hat{\beta}}{q\hat{\sigma}^2}\]
    is distributed according to a Fisher distribution with \((q, n - r)\) degrees of freedom. Each step of the reasoning must be carefully justified.

    \item Justify and construct a test of \(H_0\) against \(H_1\) of level \(\alpha\).
\end{enumerate}
\end{exercise}

\begin{exercise}[MCQ]
We have observations \((x_i, y_i) \in \mathbb{R}^2\), \(\forall i = 1, \ldots, n\). We consider the following classical Gaussian linear model:
\[Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \quad \forall i = 1, \ldots, n\]
where \((\beta_0, \beta_1) \in \mathbb{R}^2\) and \(\varepsilon_i \sim N(0, \sigma^2)\) are i.i.d.

Let \(X = \begin{pmatrix}
1 & x_1 \\
\vdots & \vdots \\
1 & x_n
\end{pmatrix}\). Assume \(X\) is a full rank matrix and note \(\hat{\beta}_0\) and \(\hat{\beta}_1\) the least squares estimators of \(\beta_0\) and \(\beta_1\).

For each of the following questions, give the answer.

\begin{enumerate}
    \item Are the variables \(Y_i\) independent and identically distributed?
    \begin{itemize}
        \item[a)] Yes \quad b) No \quad c) not always
    \end{itemize}

    \item Does the regression line calculated on the observations pass through the mean point \((\bar{x}, \bar{y})\)?
    \begin{itemize}
        \item[a)] Yes \quad b) No \quad c) Only if I am lucky
    \end{itemize}

    \item Is it possible to find estimators of \(\beta_0\) and \(\beta_1\) with smaller variance than the ordinary least squares estimators?
    \begin{itemize}
        \item[a)] Yes \quad b) No \quad c) Maybe.
    \end{itemize}

    \item Are \(\hat{\beta}_0\) and \(\hat{\beta}_1\) independent?
    \begin{itemize}
        \item[a)] Yes \quad b) No \quad c) It depends on the matrix \(X\)
    \end{itemize}

    \item If the coefficient of determination \(R^2\) calculated on the observations is equal to 1, are the points \((x_i, y_i)_{i=1,\ldots,n}\) aligned?
    \begin{itemize}
        \item[a)] Yes \quad b) No \quad c) Not necessarily
    \end{itemize}

    \item Are \(\hat{Y}\) and \(Y - \hat{Y}\) independent?
    \begin{itemize}
        \item[a)] Yes \quad b) No \quad c) It depends on the matrix \(X\)
    \end{itemize}

    \item Are \(\bar{Y} = \frac{\sum_{i=1}^n Y_i}{n}\) and \(Y - \hat{Y}\) independent?
    \begin{itemize}
        \item[a)] Yes \quad b) No \quad c) It depends on the matrix \(X\)
    \end{itemize}

    \item Is the maximum likelihood estimator of \(\sigma^2\) unbiased?
    \begin{itemize}
        \item[a)] Yes \quad b) No \quad c) We don't know
    \end{itemize}
\end{enumerate}
\end{exercise}

\begin{exercise}[This exercise will be solved without the tools of linear algebra]
Let \((x_1, y_1), \ldots, (x_n, y_n)\) be \(n\) pairs of real numbers. We suppose that \(y_i\) are the realization of \(Y_i\) whose law is given by the following equation:
\[Y_i = a + bx_i + \varepsilon_i, \quad \varepsilon_i \sim_{i.i.d.} N(0, \sigma^2)\]

\begin{enumerate}
    \item Determine \(\hat{A}\) and \(\hat{B}\) the maximum likelihood estimators of \(a\) and \(b\). Interpret the estimators.

    \item Show that these estimators are unbiased.

    \item Calculate the variance of the estimators \(\text{Var}_{\beta}(\hat{A})\) and \(\text{Var}_{\beta}(\hat{B})\). How do these variances vary as a function of \(\sigma^2\) and the experimental design \(x_1, \ldots, x_n\)?

    \item Compute the covariance of \(\hat{A}\) and \(\hat{B}\). Comment.

    \item Let \(\hat{Y}_i = \hat{A} + \hat{B}x_i\) and \(\hat{\varepsilon}_i = Y_i - \hat{Y}_i\). Show that \(\sum_{i=1}^n \hat{\varepsilon}_i = 0\).

    \item Show that \(\frac{\sum_{i=1}^n \hat{\varepsilon}_i^2}{n-2}\) is an unbiased estimator of \(\sigma^2\).

    \item Let \(x_{n+1}\) be another value. We define \(\hat{Y}_{n+1} = \hat{A} + \hat{B}x_{n+1}\). Compute the variance of this prediction.

    \item Furthermore, let \(Y_{n+1} = A + Bx_{n+1} + \varepsilon_{n+1}\). Calculate the variance of \(\hat{\varepsilon}_{n+1} = Y_{n+1} - \hat{Y}_{n+1}\). Compare it to the variance of \(\varepsilon_i\) (for \(i = 1, \ldots, n\)).

    \item Gauss-Markov Theorem:
    \begin{enumerate}
        \item[(a)] Show that \(\hat{B}\) is written as a linear combination of the observations (we will explain the weights).

        \item[(b)] Consider \(\tilde{B} = \sum_{i=1}^n \lambda_i Y_i\) another unbiased estimator of \(B\), written as a linear combination of \(Y_i\). Show that \(\sum_{i=1}^n \lambda_i = 0\) and \(\sum_{i=1}^n \lambda_i x_i = 1\).

        \item[(c)] Deduce that \(\text{Var}_{\beta}(\tilde{B}) \geq \text{Var}_{\beta}(\hat{B})\)
    \end{enumerate}
\end{enumerate}
\end{exercise}