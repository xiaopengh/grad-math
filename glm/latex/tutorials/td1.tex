% TD 1: Linear regression model exercises
% Converted from PDF to LaTeX exercise format

\begin{exercise}[MCQ]
We have observations \((x_i, y_i) \in \mathbb{R}^2\), \(\forall i = 1, \ldots, n\). We consider the following classical Gaussian linear model:
\[Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, \quad \forall i = 1, \ldots, n\]
where \((\beta_0, \beta_1) \in \mathbb{R}^2\) and \(\varepsilon_i \sim N(0, \sigma^2)\) are i.i.d.

Let \(X = \begin{pmatrix}
1 & x_1 \\
\vdots & \vdots \\
1 & x_n
\end{pmatrix}\). Assume \(X\) is a full rank matrix and note \(\hat{\beta}_0\) and \(\hat{\beta}_1\) the least squares estimators of \(\beta_0\) and \(\beta_1\).

For each of the following questions, give the answer.

\begin{enumerate}
    \item Are the variables \(Y_i\) independent and identically distributed?
    \begin{itemize}
        \item[a)] Yes \quad b) No \quad c) not always
    \end{itemize}

    \item Does the regression line calculated on the observations pass through the mean point \((\bar{x}, \bar{y})\)?
    \begin{itemize}
        \item[a)] Yes \quad b) No \quad c) Only if I am lucky
    \end{itemize}

    \item Is it possible to find estimators of \(\beta_0\) and \(\beta_1\) with smaller variance than the ordinary least squares estimators?
    \begin{itemize}
        \item[a)] Yes \quad b) No \quad c) Maybe.
    \end{itemize}

    \item Are \(\hat{\beta}_0\) and \(\hat{\beta}_1\) independent?
    \begin{itemize}
        \item[a)] Yes \quad b) No \quad c) It depends on the matrix \(X\)
    \end{itemize}

    \item If the coefficient of determination \(R^2\) calculated on the observations is equal to 1, are the points \((x_i, y_i)_{i=1,\ldots,n}\) aligned?
    \begin{itemize}
        \item[a)] Yes \quad b) No \quad c) Not necessarily
    \end{itemize}

    \item Are \(\hat{Y}\) and \(Y - \hat{Y}\) independent?
    \begin{itemize}
        \item[a)] Yes \quad b) No \quad c) It depends on the matrix \(X\)
    \end{itemize}

    \item Are \(\bar{Y} = \frac{\sum_{i=1}^n Y_i}{n}\) and \(Y - \hat{Y}\) independent?
    \begin{itemize}
        \item[a)] Yes \quad b) No \quad c) It depends on the matrix \(X\)
    \end{itemize}

    \item Is the maximum likelihood estimator of \(\sigma^2\) unbiased?
    \begin{itemize}
        \item[a)] Yes \quad b) No \quad c) We don't know
    \end{itemize}
\end{enumerate}
\end{exercise}

\begin{exercise}[This exercise will be solved without the tools of linear algebra]
Let \((x_1, y_1), \ldots, (x_n, y_n)\) be \(n\) pairs of real numbers. We suppose that \(y_i\) are the realization of \(Y_i\) whose law is given by the following equation:
\[Y_i = a + bx_i + \varepsilon_i, \quad \varepsilon_i \sim_{i.i.d.} N(0, \sigma^2)\]

\begin{enumerate}
    \item Determine \(\hat{A}\) and \(\hat{B}\) the maximum likelihood estimators of \(a\) and \(b\). Interpret the estimators.

    \item Show that these estimators are unbiased.

    \item Calculate the variance of the estimators \(\text{Var}_{\beta}(\hat{A})\) and \(\text{Var}_{\beta}(\hat{B})\). How do these variances vary as a function of \(\sigma^2\) and the experimental design \(x_1, \ldots, x_n\)?

    \item Compute the covariance of \(\hat{A}\) and \(\hat{B}\). Comment.

    \item Let \(\hat{Y}_i = \hat{A} + \hat{B}x_i\) and \(\hat{\varepsilon}_i = Y_i - \hat{Y}_i\). Show that \(\sum_{i=1}^n \hat{\varepsilon}_i = 0\).

    \item Show that \(\frac{\sum_{i=1}^n \hat{\varepsilon}_i^2}{n-2}\) is an unbiased estimator of \(\sigma^2\).

    \item Let \(x_{n+1}\) be another value. We define \(\hat{Y}_{n+1} = \hat{A} + \hat{B}x_{n+1}\). Compute the variance of this prediction.

    \item Furthermore, let \(Y_{n+1} = A + Bx_{n+1} + \varepsilon_{n+1}\). Calculate the variance of \(\hat{\varepsilon}_{n+1} = Y_{n+1} - \hat{Y}_{n+1}\). Compare it to the variance of \(\varepsilon_i\) (for \(i = 1, \ldots, n\)).

    \item Gauss-Markov Theorem:
    \begin{enumerate}
        \item[(a)] Show that \(\hat{B}\) is written as a linear combination of the observations (we will explain the weights).

        \item[(b)] Consider \(\tilde{B} = \sum_{i=1}^n \lambda_i Y_i\) another unbiased estimator of \(B\), written as a linear combination of \(Y_i\). Show that \(\sum_{i=1}^n \lambda_i = 0\) and \(\sum_{i=1}^n \lambda_i x_i = 1\).

        \item[(c)] Deduce that \(\text{Var}_{\beta}(\tilde{B}) \geq \text{Var}_{\beta}(\hat{B})\)
    \end{enumerate}
\end{enumerate}
\end{exercise}