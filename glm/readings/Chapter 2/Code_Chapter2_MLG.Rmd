---
title: "Linear Models and Their Generalizations"
subtitle: "Code Chapter 2: Validation and selection model"
author: "Katia Meziani"
output:
  html_document:
    self_contained: true
    math_method: 
      engine: mathjax
      url: https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js
    code_folding: hide
    css: ./style.css
    df_print: paged
    highlight: tango
    number_sections: no
    theme: flatly
    toc: yes
    toc_float:
      collapsed: no
---





```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE,message=FALSE)
```








# 1. Multivariate linear gaussian regression model

##   The data set Ozone

```{r}
library(rmarkdown)
library(dplyr)
ozone1 <- read.table("ozone1.txt",header=TRUE, sep="", dec=",")
ozone1$Date <- as.Date(as.factor(ozone1$Date),format = "%Y%m%d")
ozone1$vent<-as.factor(ozone1$vent)
ozone1$pluie<-as.factor(ozone1$pluie)
ozone1 <- ozone1 %>% mutate(across(where(is.character), as.numeric))
ozone1 <- ozone1 %>% mutate(across(where(is.integer), as.numeric))
ozone1  <- ozone1  %>% arrange(Date) 
Ozone<-ozone1[,2:12]
paged_table(Ozone)
```


##   Train/Test Split




In the case of data collected over several consecutive days (our setting), this type of train/test split can introduce biases, especially if the measurements are time-correlated. In this case, it is better to perform a chronological split, meaning using the earlier observations for training and the later ones for testing.

```{r}
train_size <- floor(0.8 * nrow(Ozone))

train <- Ozone[1:train_size, ]
test <- Ozone[(train_size + 1):nrow(Ozone), ]
```






##   Declaration of the model



```{r}
mod<-lm(maxO3~.,data=train)
summary(mod)
```



# 2. Model validation

##   Overview of Residuals


Display the `summary of the  estimates residuals.
 
 
```{r}
summary(residuals(mod))
```



 Display the `summary of the  standardized residuals.

```{r}
summary(rstandard(mod))

```

 Display the `summary of the  studentized residuals.

```{r}
summary(rstudent(mod))
```





##   Practical model validation


**Residuals vs Fitted plot**

```{r  fig.height = 3, fig.align = 'center', message = FALSE, warning = FALSE}
library(ggfortify)
autoplot(mod,1)
```


**Scale-location plot**

```{r  fig.height = 3, fig.align = 'center', message = FALSE, warning = FALSE}
autoplot(mod,3)
```

 **Breusch-Pagan** test 

$$\mathscr{H}_0: \,\text{"Errors are homoscedastic"} \ vs \ \mathscr{H}_1: \,\text{"Errors are heteroscedastic"}
$$


```{r}
library(car)
ncvTest(mod)
```






**Durbin-Watson Test**  ($H_0 : \rho_1 = 0$)

```{r}
set.seed(0)
durbinWatsonTest(mod)
```



**Normal QQplot**

```{r  fig.height = 3, fig.align = 'center', message = FALSE, warning = FALSE}
autoplot(mod,2)
```

**Shapiro** test available to make a decision.
$$\mathscr{H}_0: \,\text{"Errors are normally distributed"} \ vs \ \mathscr{H}_1: \,\text{"Errors are not normally distributed"}
$$

```{r}
shapiro.test(residuals(mod))
```



# 3. Selection Model



##   Impacts of High Correlation Between Variables




**Display the correlation plot.**

```{r  fig.align = 'center'}
library(dplyr)
library(corrplot)
correlation_matrix<-cor(Ozone)
corrplot(correlation_matrix, order = 'hclust',addrect = 3)
```






**For further exploration.** If you want to explore pairwise relationships beyond correlations (e.g., for both numeric and categorical variables), the `ggpairs()` function from the `GGally` package can provide scatter plots, correlations, density plots and more detailed pairwise visualization


```{r  fig.align = 'center'}
library(GGally)
ggpairs(Ozone)
```



# 5. Step-by-step method



**Declare our full model `mod` and the model reduced to the intercept `mod0`**

```{r}
mod0<-lm(maxO3~1,data=train)
mod<-lm(maxO3~.,data=train)

```





##   Forward selection


```{r}
library(MASS)
mod_forw<-stepAIC(mod0, maxO3~T9+T12+T15+Ne9+Ne12+Ne15+Vx9+Vx12+Vx15+maxO3v,data=train,trace=T,direction=c('forward'))

mod_forw
```
While with criterion BIC,  we select the following model

```{r}
n<-length(train$maxO3)
mod_forw_BIC<-stepAIC(mod0, maxO3~T9+T12+T15+Ne9+Ne12+Ne15+Vx9+Vx12+Vx15+maxO3v,data=train,trace=F,k=log(n),direction=c('forward'))
mod_forw_BIC
```

##   Backard selection



```{r}
library(MASS)
mod_back<-stepAIC(mod,~.,trace=T,direction=c("backward"))

mod_back
```

While with criterion BIC,  we select the following model

```{r}
library(MASS)
stepAIC(mod,~.,trace=F,k=log(n),direction=c("backward"))

```


##   Stepwise selection/both selection 




```{r}
library(MASS)
mod_both<-stepAIC(mod0, maxO3~T9+T12+T15+Ne9+Ne12+Ne15+Vx9+Vx12+Vx15+maxO3v,data=train,trace=T,direction=c('both'))

mod_both
```


While with criterion BIC,  we select the following model

```{r}
library(MASS)
stepAIC(mod0, maxO3~T9+T12+T15+Ne9+Ne12+Ne15+Vx9+Vx12+Vx15+maxO3v,data=train,trace=F,k=log(n), direction=c('both'))


```



##   Discussion

Compute theis model and the full model by
 a Fisher test


```{r}


mod_0 <- mod_forw
mod_1 <- mod

anova(mod_0, mod_1)

```

 


```{r}
library(Metrics)
mod<-lm(maxO3~.,data=train)
RMSE_full_train= rmse(train$maxO3,predict(mod, newdata = train))
RMSE_full_test= rmse(test$maxO3,predict(mod, newdata = test))

RMSE_0_train= rmse(train$maxO3,predict(mod_0, newdata = train))
RMSE_0_test= rmse(test$maxO3,predict(mod_0, newdata = test))



RMSE_train<-c(RMSE_full_train,RMSE_0_train)
RMSE_test<-c(RMSE_full_test,RMSE_0_test)
RMSE<-rbind.data.frame(RMSE_train,RMSE_test)
names(RMSE)<-c('mod','mod_0')
row.names(RMSE)<-c('Train','Test')
RMSE
```






# 6. Outliers




##   Regression outliers



These observations can be identified on the *Studentized-plot*.


```{r out.width = "75%",message=FALSE}
library(car)
influenceIndexPlot(mod_0,vars="Studentized")

```


 


##   Outliers with strong leverage on themselves

 These observations can be identified on the *Hat-plot*."




```{r out.width = "75%",message=FALSE}
influenceIndexPlot(mod_0,vars="hat")

```


 



##   Outliers with strong leverage on the model

 

These outliers can be identified on the *Cook's-plot*."


```{r out.width = "75%",message=FALSE}
influenceIndexPlot(mod_0,vars="cook")

```

 
<span class="solution">üìù</span> Note also that you can get this plot with `√†utoplot(mod_0,1:4)`

```{r out.width = "75%",message=FALSE}
library(ggfortify)
autoplot(mod_0,1:4)

```



## 6.5. Handling of outliers



 
The `outlierTest()` function in R (from the `car` package) is designed to identify **outliers** in the context of a linear regression model. It is based on the analysis of **studentized residuals**, taking into account the influence of other data points in the model
 $$\mathscr{H}_0 \ : \  \text{No outliers exist.}  \qquad vs  \qquad \mathscr{H}_1 \ : \  \text{Observation i is an outlier if }t^*_i \ \text{is significant.}$$
 
 

 


The `outlierTest()` returns observations with significantly small p-values after adjustment, indicating that these observations are outliers.
```{r}
library(car)
outlierTest(mod_0)
```

 


 

<span class="solution">‚úç</span>  Note that you can display 4 plot with  `influenceIndexPlot(mod_0)`



```{r out.width = "75%",message=FALSE}
influenceIndexPlot(mod_0)
```







