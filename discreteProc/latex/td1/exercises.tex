\begin{exercise}
Let $X$ and $Y$ be two independent random variables with Poisson distribution of parameters $\lambda$ and $\mu$ respectively.

1. What is the distribution of $X+Y$?  

2. Compute $\mathbb{E}(X \mid X+Y)$.
\end{exercise}

\input{answers/exo1.tex}

\begin{exercise}
Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space and $\{A_1, \ldots, A_n\}$ be a finite partition of $\Omega$. We define $\mathcal{G} = \sigma(A_1, \ldots, A_n)$ the $\sigma$-algebra generated by this partition.  

1. Describe the $\sigma$-field $\mathcal{G}$.  

2. Let $X$ be an integrable random variable. Show that
\[
\mathbb{E}(X \mid \mathcal{G})(\omega) = \sum_{j:\,\mathbb{P}(A_j) > 0} \frac{\mathbb{E}(X1_{A_j})}{\mathbb{P}(A_j)} 1_{A_j}(\omega).
\]
\end{exercise}

\input{answers/exo2.tex}

\begin{exercise}
Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space and $\mathcal{G}$ be a sub-$\sigma$-algebra of $\mathcal{F}$. Let $X$ and $Y$ be two square integrable random variables. Show that
\[
\mathbb{E}(X \, \mathbb{E}(Y \mid \mathcal{G})) = \mathbb{E}(Y \, \mathbb{E}(X \mid \mathcal{G})).
\]
\end{exercise}

\begin{exercise}
Let $X_1, \ldots, X_n$ be i.i.d. integrable random variables. Determine the following conditional expectations:  

1. $\mathbb{E}[X_1 + X_2 + \cdots + X_n \mid X_1]$,  

2. $\mathbb{E}[X_1 \mid X_1 + X_2 + \cdots + X_n]$.
\end{exercise}

\begin{exercise}
1. Let $X,Y$ be two i.i.d. random variables uniformly distributed on $[0,1]$. Compute $\mathbb{E}(X \mid XY)$.  

2. Let $X \sim \mathcal{N}(0,1)$. Compute $\mathbb{E}(X^2 \mid X)$, $\mathbb{E}(X \mid X^2)$ and $\mathbb{E}(X^3 \mid X^2)$.  

3. Let $X$ and $Y$ be i.i.d. random variables uniformly distributed on $[-\pi/2, \pi/2]$. Compute
\[
\mathbb{E}(\sin X \mid \cos X), \quad \mathbb{E}(X \mid e^X), \quad \mathbb{E}(\cos X \mid \sin Y), \quad \mathbb{E}(\sin X \mid \cos(X+2Y)).
\]
\end{exercise}

\begin{exercise}
Let $(X,Y)$ be a random vector with density
\[
p_{X,Y}(x,y) = \alpha \beta \, y \, \exp\!\left(-\frac{\alpha x}{y} - \beta y\right) 1_{x>0} 1_{y>0},
\]
where $\alpha > 0$ and $\beta > 0$ are parameters. Determine $\mathbb{E}(X \mid Y)$ and deduce $\mathbb{E}(X)$.
\end{exercise}

\begin{exercise}
Let $Z$ be a random variable exponentially distributed with parameter $1$ and let $t > 0$. We set $X = \min(Z,t)$ and $Y = \max(Z,t)$. Compute $\mathbb{E}[Z \mid X]$ and $\mathbb{E}[Z \mid Y]$.
\end{exercise}

\begin{exercise}
Let $X$ be a square integrable random variable defined on a probability space $(\Omega,\mathcal{F},\mathbb{P})$ and let $\mathcal{G}$ a sub-$\sigma$-algebra of $\mathcal{F}$. We set
\[
\mathrm{var}(X \mid \mathcal{G}) = \mathbb{E}[X^2 \mid \mathcal{G}] - \mathbb{E}[X \mid \mathcal{G}]^2.
\]
Show that
\[
\mathrm{var}(X) = \mathbb{E}[\mathrm{var}(X \mid \mathcal{G})] + \mathrm{var}(\mathbb{E}[X \mid \mathcal{G}]).
\]
\end{exercise}

\begin{exercise}
Let $(X_0, X_1, \ldots, X_n)$ be a Gaussian random vector with mean zero and nondegenerate covariance matrix $\Gamma$. Show that there exist real numbers $\lambda_1, \ldots, \lambda_n$ such that
\[
\mathbb{E}[X_0 \mid X_1, \ldots, X_n] = \sum_{i=1}^n \lambda_i X_i,
\]
and determine the weights $\lambda_i$ as a function of $\Gamma$.  

\textit{Hint:} The coordinates of a Gaussian vector are independent if and only if their covariance is zero.
\end{exercise}
