% TD 1: Conditional Expectation
% Converted from PDF to LaTeX exercise format

\begin{exercise}
Let \(X\) and \(Y\) be two independent random variables with Poisson distribution of parameters \(\lambda\) and \(\mu\) respectively.
\begin{enumerate}
    \item What is the distribution of \(X + Y\)?
    \item Compute \(E(X|X + Y)\).
\end{enumerate}
\end{exercise}

\begin{exercise}
Let \((\Omega, \mathcal{F}, P)\) be a probability space and \(\{A_1, \ldots, A_n\}\) be a finite partition of \(\Omega\). We define \(\mathcal{G} = \sigma(A_1, \ldots, A_n)\) the \(\sigma\)-algebra generated by this partition.
\begin{enumerate}
    \item Describe the \(\sigma\)-field \(\mathcal{G}\).
    \item Let \(X\) be an integrable random variable. Show that
    \[E(X|\mathcal{G})(\omega) = \sum_{j:P(A_j)>0} \frac{E(X\mathbf{1}_{A_j})}{P(A_j)} \mathbf{1}_{A_j}(\omega).\]
\end{enumerate}
\end{exercise}

\begin{exercise}
Let \((\Omega, \mathcal{F}, P)\) be a probability space and \(\mathcal{G}\) be a sub-\(\sigma\)-algebra of \(\mathcal{F}\). Let \(X\) and \(Y\) be two square integrable random variables. Show that
\[E(XE(Y|\mathcal{G})) = E(YE(X|\mathcal{G})).\]
\end{exercise}

\begin{exercise}
Let \(X_1, \ldots, X_n\) be i.i.d. integrable random variables. Determine the following conditional expectations:
\begin{enumerate}
    \item \(E[X_1 + X_2 + \cdots + X_n | X_1]\),
    \item \(E[X_1 | X_1 + X_2 + \cdots + X_n]\).
\end{enumerate}
\end{exercise}

\begin{exercise}
\begin{enumerate}
    \item Let \(X, Y\) be two i.i.d. random variables uniformly distributed on \([0, 1]\). Compute \(E(X|XY)\).

    \item Let \(X \sim N(0, 1)\). Compute \(E(X^2|X)\), \(E(X|X^2)\) and \(E(X^3|X^2)\).

    \item Let \(X\) and \(Y\) be i.i.d. random variables uniformly distributed on \([-\pi/2, \pi/2]\). Compute
    \[E(\sin X| \cos X), \quad E\left(X|e^X\right),\]
    \[E(\cos X|\sin Y), \quad E(\sin X| \cos(X + 2Y)).\]
\end{enumerate}
\end{exercise}

\begin{exercise}
Let \((X, Y)\) be a random vector with density
\[p_{X,Y}(x, y) = \frac{\alpha\beta}{y} \exp\left\{-\frac{\alpha x}{y} - \beta y\right\}\mathbf{1}_{x>0}\mathbf{1}_{y>0},\]
where \(\alpha > 0\) and \(\beta > 0\) are parameters. Determine \(E(X|Y)\) and deduce \(E(X)\).
\end{exercise}

\begin{exercise}
Let \(Z\) be a random variable exponentially distributed with parameter 1 and let \(t > 0\). We set \(X = \min(Z, t)\) and \(Y = \max(Z, t)\). Compute \(E[Z | X]\) and \(E[Z | Y]\).
\end{exercise}

\begin{exercise}
Let \(X\) be a square integrable random variable defined on a probability space \((\Omega, \mathcal{F}, P)\) and let \(\mathcal{G}\) a sub-\(\sigma\)-algebra of \(\mathcal{F}\). We set
\[\text{var}(X | \mathcal{G}) = E[X^2| \mathcal{G}] - E[X | \mathcal{G}]^2.\]
Show that
\[\text{var}(X) = E[\text{var}(X | \mathcal{G})] + \text{var}(E[X | \mathcal{G}]).\]
\end{exercise}

\begin{exercise}
Let \((X_0, X_1, \ldots, X_n)\) be a Gaussian random vector with mean zero and nondegenerate covariance matrix \(\Gamma\). Show that there exist real numbers \(\lambda_1, \ldots, \lambda_n\) such that
\[E[X_0 | X_1, \ldots, X_n] = \sum_{i=1}^n \lambda_i X_i\]
and determine the weights \(\lambda_i\) as a function of \(\Gamma\).

\textit{Hint:} The coordinates of a Gaussian vector are independent if and only if their covariance is zero.
\end{exercise}